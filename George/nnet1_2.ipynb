{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import load, get_audio_path, stereo_to_mono, frequency_mask, time_mask, plot_spectrogram\n",
    "# define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './fma/data/fma_small'\n",
    "\n",
    "tracks = load('fma/data/fma_metadata/tracks.csv')\n",
    "#features = utils.load('fma/data/fma_metadata/features.csv')\n",
    "#echonest = utils.load('fma/data/fma_metadata/echonest.csv')\n",
    "\n",
    "subset = tracks.index[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "#features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "#features_all = features.loc[subset]\n",
    "\n",
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMA2D(Dataset):   \n",
    "    \n",
    "    \n",
    "    def __init__(self, track_ids, use_subsamples=True, transforms=None, augment_prob=0.5):\n",
    "        self.track_ids = track_ids\n",
    "        self.subsamples = use_subsamples\n",
    "        self.transforms = transforms\n",
    "        self.augment_prob = augment_prob\n",
    "        self.data_path = './fma/data/spectrograms_2/' if use_subsamples else './fma/data/spectrograms_1/'\n",
    "        self.data = []\n",
    "        if use_subsamples:\n",
    "            for tid in track_ids:\n",
    "                for i in range(7):\n",
    "                    self.data.append([self.data_path +\"{:06d}\".format(tid)+f'_{i}.pt', tid])\n",
    "        else:\n",
    "            for tid in track_ids:\n",
    "                self.data.append([self.data_path +\"{:06d}\".format(tid)+'.pt', tid])\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        \n",
    "        spec_path = self.data[index][0]\n",
    "        tid = self.data[index][1]\n",
    "        \n",
    "        # load the spectrogram data\n",
    "        try:\n",
    "            spec = torch.load(spec_path)\n",
    "        except Exception as e:\n",
    "            return self.__getitem__(index + 1)\n",
    "        \n",
    "        if self.transforms and torch.rand(1) < self.augment_prob:\n",
    "            spec = self.transforms[0](spec)\n",
    "            spec = self.transforms[1](spec)\n",
    "            \n",
    "        \n",
    "        label = torch.from_numpy(labels_onehot.loc[tid].values).float()\n",
    "        return spec, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "class AudioClassifier (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=8)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet1(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(4, input_size), stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),   \n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(4, 1), stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(4, 1), stride=1, padding=2, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(26, 1))\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(26, 1))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(6656, 300)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear2 = nn.Linear(300, 150 )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(150, num_classes)\n",
    "        \n",
    "        \n",
    "     #in this method we tell pytorch how to pass data from layer to another layer   \n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        y = self.maxpool(x)\n",
    "        z = self.avgpool(x)\n",
    "        x = torch.cat((y, z), dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_size, F=256):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels = 1, out_channels = F, kernel_size = (4, input_size), bias = False),\n",
    "                        nn.BatchNorm2d(F),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels = F, out_channels = F, kernel_size = (4, 1), bias = False),\n",
    "                        nn.BatchNorm2d(F),\n",
    "                        nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels = F, out_channels = F, kernel_size = (4, 1),  padding=(3,0), bias = False),\n",
    "                        nn.BatchNorm2d(F))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        out = self.conv2(residual)\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out + residual)\n",
    "        return out       \n",
    "\n",
    "\n",
    "class nnet2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=8, F=256):\n",
    "        super(nnet2, self).__init__()\n",
    "        self.block = ResidualBlock(input_size, F=F)\n",
    "        self.Max = nn.MaxPool2d(kernel_size = (125, 1))\n",
    "        self.Avg = nn.AvgPool2d(kernel_size = (125, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(F*2, F)\n",
    "        self.linear2 = nn.Linear(F,  F//2)\n",
    "        self.linear3 = nn.Linear(F//2, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        #print(x.size())\n",
    "        y = self.Max(x)\n",
    "        z = self.Avg(x)\n",
    "        #print(y.size(), z.size())\n",
    "        x = torch.cat((y, z), dim=1)\n",
    "        #print(x.size())\n",
    "        x = self.flatten(x)\n",
    "        #print(x.size())\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet34\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1 #we don't use the block.expansion here\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1,padding = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size = 3, stride=stride,\n",
    "                     padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3, stride=1,\n",
    "                     padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes))\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inplanes = 128\n",
    "\n",
    "        self.c1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 128, 128, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, 256, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
    "        self.fc = nn.Linear(25088 , num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    " \n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride))\n",
    "        \n",
    "        self.inplanes = planes\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)           \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)         \n",
    "\n",
    "        x = self.layer1(x)          \n",
    "        x = self.layer2(x)          \n",
    "        x = self.layer3(x)          \n",
    "        x = self.layer4(x)          \n",
    "\n",
    "        x = self.avgpool(x)         \n",
    "        x = torch.flatten(x, 1)     \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def resnet34():\n",
    "    layers=[3, 4, 6, 3]\n",
    "    model = ResNet(BasicBlock, layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetM(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, p_dropout=0.5, num_classes=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = 'ResNet2D-M'\n",
    "        self.inplanes = 16\n",
    "\n",
    "        self.c1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 16, 32, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 32, 64, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 64, 128, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
    "        self.fc = nn.Linear(6272 , num_classes)\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    " \n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride))\n",
    "        \n",
    "        self.inplanes = planes\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)           \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)         \n",
    "\n",
    "        x = self.layer1(x)          \n",
    "        x = self.layer2(x)          \n",
    "        x = self.layer3(x)          \n",
    "        x = self.layer4(x)          \n",
    "\n",
    "        x = self.avgpool(x)         \n",
    "        x = torch.flatten(x, 1)     \n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def resnet34M(p_dropout=0.5):\n",
    "    layers=[3, 4, 6, 3]\n",
    "    model = ResNetM(BasicBlock, layers, p_dropout=p_dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 1290])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 64, 645]             784\n",
      "       BatchNorm2d-2          [-1, 16, 64, 645]              32\n",
      "              ReLU-3          [-1, 16, 64, 645]               0\n",
      "         MaxPool2d-4          [-1, 16, 32, 323]               0\n",
      "            Conv2d-5          [-1, 32, 32, 323]           4,608\n",
      "              ReLU-6          [-1, 32, 32, 323]               0\n",
      "            Conv2d-7          [-1, 32, 32, 323]           9,216\n",
      "       BatchNorm2d-8          [-1, 32, 32, 323]              64\n",
      "            Conv2d-9          [-1, 32, 32, 323]             512\n",
      "      BatchNorm2d-10          [-1, 32, 32, 323]              64\n",
      "             ReLU-11          [-1, 32, 32, 323]               0\n",
      "       BasicBlock-12          [-1, 32, 32, 323]               0\n",
      "           Conv2d-13          [-1, 32, 32, 323]           9,216\n",
      "             ReLU-14          [-1, 32, 32, 323]               0\n",
      "           Conv2d-15          [-1, 32, 32, 323]           9,216\n",
      "      BatchNorm2d-16          [-1, 32, 32, 323]              64\n",
      "           Conv2d-17          [-1, 32, 32, 323]           1,024\n",
      "      BatchNorm2d-18          [-1, 32, 32, 323]              64\n",
      "             ReLU-19          [-1, 32, 32, 323]               0\n",
      "       BasicBlock-20          [-1, 32, 32, 323]               0\n",
      "           Conv2d-21          [-1, 32, 32, 323]           9,216\n",
      "             ReLU-22          [-1, 32, 32, 323]               0\n",
      "           Conv2d-23          [-1, 32, 32, 323]           9,216\n",
      "      BatchNorm2d-24          [-1, 32, 32, 323]              64\n",
      "           Conv2d-25          [-1, 32, 32, 323]           1,024\n",
      "      BatchNorm2d-26          [-1, 32, 32, 323]              64\n",
      "             ReLU-27          [-1, 32, 32, 323]               0\n",
      "       BasicBlock-28          [-1, 32, 32, 323]               0\n",
      "           Conv2d-29          [-1, 32, 16, 162]           9,216\n",
      "             ReLU-30          [-1, 32, 16, 162]               0\n",
      "           Conv2d-31          [-1, 32, 16, 162]           9,216\n",
      "      BatchNorm2d-32          [-1, 32, 16, 162]              64\n",
      "           Conv2d-33          [-1, 32, 16, 162]           1,024\n",
      "      BatchNorm2d-34          [-1, 32, 16, 162]              64\n",
      "             ReLU-35          [-1, 32, 16, 162]               0\n",
      "       BasicBlock-36          [-1, 32, 16, 162]               0\n",
      "           Conv2d-37          [-1, 32, 16, 162]           9,216\n",
      "             ReLU-38          [-1, 32, 16, 162]               0\n",
      "           Conv2d-39          [-1, 32, 16, 162]           9,216\n",
      "      BatchNorm2d-40          [-1, 32, 16, 162]              64\n",
      "           Conv2d-41          [-1, 32, 16, 162]           1,024\n",
      "      BatchNorm2d-42          [-1, 32, 16, 162]              64\n",
      "             ReLU-43          [-1, 32, 16, 162]               0\n",
      "       BasicBlock-44          [-1, 32, 16, 162]               0\n",
      "           Conv2d-45          [-1, 32, 16, 162]           9,216\n",
      "             ReLU-46          [-1, 32, 16, 162]               0\n",
      "           Conv2d-47          [-1, 32, 16, 162]           9,216\n",
      "      BatchNorm2d-48          [-1, 32, 16, 162]              64\n",
      "           Conv2d-49          [-1, 32, 16, 162]           1,024\n",
      "      BatchNorm2d-50          [-1, 32, 16, 162]              64\n",
      "             ReLU-51          [-1, 32, 16, 162]               0\n",
      "       BasicBlock-52          [-1, 32, 16, 162]               0\n",
      "           Conv2d-53          [-1, 32, 16, 162]           9,216\n",
      "             ReLU-54          [-1, 32, 16, 162]               0\n",
      "           Conv2d-55          [-1, 32, 16, 162]           9,216\n",
      "      BatchNorm2d-56          [-1, 32, 16, 162]              64\n",
      "           Conv2d-57          [-1, 32, 16, 162]           1,024\n",
      "      BatchNorm2d-58          [-1, 32, 16, 162]              64\n",
      "             ReLU-59          [-1, 32, 16, 162]               0\n",
      "       BasicBlock-60          [-1, 32, 16, 162]               0\n",
      "           Conv2d-61            [-1, 64, 8, 81]          18,432\n",
      "             ReLU-62            [-1, 64, 8, 81]               0\n",
      "           Conv2d-63            [-1, 64, 8, 81]          36,864\n",
      "      BatchNorm2d-64            [-1, 64, 8, 81]             128\n",
      "           Conv2d-65            [-1, 64, 8, 81]           2,048\n",
      "      BatchNorm2d-66            [-1, 64, 8, 81]             128\n",
      "             ReLU-67            [-1, 64, 8, 81]               0\n",
      "       BasicBlock-68            [-1, 64, 8, 81]               0\n",
      "           Conv2d-69            [-1, 64, 8, 81]          36,864\n",
      "             ReLU-70            [-1, 64, 8, 81]               0\n",
      "           Conv2d-71            [-1, 64, 8, 81]          36,864\n",
      "      BatchNorm2d-72            [-1, 64, 8, 81]             128\n",
      "           Conv2d-73            [-1, 64, 8, 81]           4,096\n",
      "      BatchNorm2d-74            [-1, 64, 8, 81]             128\n",
      "             ReLU-75            [-1, 64, 8, 81]               0\n",
      "       BasicBlock-76            [-1, 64, 8, 81]               0\n",
      "           Conv2d-77            [-1, 64, 8, 81]          36,864\n",
      "             ReLU-78            [-1, 64, 8, 81]               0\n",
      "           Conv2d-79            [-1, 64, 8, 81]          36,864\n",
      "      BatchNorm2d-80            [-1, 64, 8, 81]             128\n",
      "           Conv2d-81            [-1, 64, 8, 81]           4,096\n",
      "      BatchNorm2d-82            [-1, 64, 8, 81]             128\n",
      "             ReLU-83            [-1, 64, 8, 81]               0\n",
      "       BasicBlock-84            [-1, 64, 8, 81]               0\n",
      "           Conv2d-85            [-1, 64, 8, 81]          36,864\n",
      "             ReLU-86            [-1, 64, 8, 81]               0\n",
      "           Conv2d-87            [-1, 64, 8, 81]          36,864\n",
      "      BatchNorm2d-88            [-1, 64, 8, 81]             128\n",
      "           Conv2d-89            [-1, 64, 8, 81]           4,096\n",
      "      BatchNorm2d-90            [-1, 64, 8, 81]             128\n",
      "             ReLU-91            [-1, 64, 8, 81]               0\n",
      "       BasicBlock-92            [-1, 64, 8, 81]               0\n",
      "           Conv2d-93            [-1, 64, 8, 81]          36,864\n",
      "             ReLU-94            [-1, 64, 8, 81]               0\n",
      "           Conv2d-95            [-1, 64, 8, 81]          36,864\n",
      "      BatchNorm2d-96            [-1, 64, 8, 81]             128\n",
      "           Conv2d-97            [-1, 64, 8, 81]           4,096\n",
      "      BatchNorm2d-98            [-1, 64, 8, 81]             128\n",
      "             ReLU-99            [-1, 64, 8, 81]               0\n",
      "      BasicBlock-100            [-1, 64, 8, 81]               0\n",
      "          Conv2d-101            [-1, 64, 8, 81]          36,864\n",
      "            ReLU-102            [-1, 64, 8, 81]               0\n",
      "          Conv2d-103            [-1, 64, 8, 81]          36,864\n",
      "     BatchNorm2d-104            [-1, 64, 8, 81]             128\n",
      "          Conv2d-105            [-1, 64, 8, 81]           4,096\n",
      "     BatchNorm2d-106            [-1, 64, 8, 81]             128\n",
      "            ReLU-107            [-1, 64, 8, 81]               0\n",
      "      BasicBlock-108            [-1, 64, 8, 81]               0\n",
      "          Conv2d-109           [-1, 128, 4, 41]          73,728\n",
      "            ReLU-110           [-1, 128, 4, 41]               0\n",
      "          Conv2d-111           [-1, 128, 4, 41]         147,456\n",
      "     BatchNorm2d-112           [-1, 128, 4, 41]             256\n",
      "          Conv2d-113           [-1, 128, 4, 41]           8,192\n",
      "     BatchNorm2d-114           [-1, 128, 4, 41]             256\n",
      "            ReLU-115           [-1, 128, 4, 41]               0\n",
      "      BasicBlock-116           [-1, 128, 4, 41]               0\n",
      "          Conv2d-117           [-1, 128, 4, 41]         147,456\n",
      "            ReLU-118           [-1, 128, 4, 41]               0\n",
      "          Conv2d-119           [-1, 128, 4, 41]         147,456\n",
      "     BatchNorm2d-120           [-1, 128, 4, 41]             256\n",
      "          Conv2d-121           [-1, 128, 4, 41]          16,384\n",
      "     BatchNorm2d-122           [-1, 128, 4, 41]             256\n",
      "            ReLU-123           [-1, 128, 4, 41]               0\n",
      "      BasicBlock-124           [-1, 128, 4, 41]               0\n",
      "          Conv2d-125           [-1, 128, 4, 41]         147,456\n",
      "            ReLU-126           [-1, 128, 4, 41]               0\n",
      "          Conv2d-127           [-1, 128, 4, 41]         147,456\n",
      "     BatchNorm2d-128           [-1, 128, 4, 41]             256\n",
      "          Conv2d-129           [-1, 128, 4, 41]          16,384\n",
      "     BatchNorm2d-130           [-1, 128, 4, 41]             256\n",
      "            ReLU-131           [-1, 128, 4, 41]               0\n",
      "      BasicBlock-132           [-1, 128, 4, 41]               0\n",
      "AdaptiveAvgPool2d-133            [-1, 128, 7, 7]               0\n",
      "          Linear-134                    [-1, 8]          50,184\n",
      "         Dropout-135                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 1,484,472\n",
      "Trainable params: 1,484,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.63\n",
      "Forward/backward pass size (MB): 116.27\n",
      "Params size (MB): 5.66\n",
      "Estimated Total Size (MB): 122.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "BATCH = 32\n",
    "EPOCHS = 40\n",
    "augment_prob = 0.8\n",
    "\n",
    "# create a training dataset and dataloader\n",
    "dataset = FMA2D(train, use_subsamples=False, transforms=[frequency_mask, time_mask], augment_prob=augment_prob)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "# create a validation dataset and dataloader\n",
    "val_dataset = FMA2D(val, use_subsamples=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer01\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer_name = 'Adam'\n",
    "\n",
    "# Lee 2017\n",
    "# SGD optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5)\n",
    "\n",
    "\n",
    "for spec, label in val_dataloader:\n",
    "    print(spec.size())\n",
    "    #plot_spectrogram(spec[0])\n",
    "    input_size = spec.size()[2]\n",
    "    break\n",
    "\n",
    "p_dropout = 0.3\n",
    "model = resnet34M(p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "summary(model, (1, 128, 1290))\n",
    "\n",
    "\n",
    "timestamp = time.strftime(\"feb%d_t%H%M\", time.gmtime())\n",
    "model_name = f\"{model.name}_B{BATCH}_E{EPOCHS}_O{optimizer_name}_LR{lr}_pD{p_dropout}_A{augment_prob}_{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  6400 subsamples] Training loss: 2.146\n",
      "Validation Loss: 2.1389 | Validation Accuracy: 0.1288 | Training Accuracy: 0.1339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Update the learning rate\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# scheduler.step(loss)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m _, train_predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "best_val_loss = float('inf') # initialize the best validation loss\n",
    "\n",
    "\n",
    "# train the model\n",
    "acc_tr = []\n",
    "acc_val = []\n",
    "loss_tr = []\n",
    "loss_val = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # evaluate the model on the training dataset\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for spectrogram, label in dataloader:\n",
    "            model.train()\n",
    "            label = label.to(device)\n",
    "            train_label = torch.argmax(label, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "            # forward pass\n",
    "            spectrogram = spectrogram.squeeze(0)  \n",
    "            spectrogram = spectrogram.unsqueeze(1)\n",
    "            \n",
    "            spectrogram = spectrogram.to(device)\n",
    "            output = model(spectrogram)\n",
    "            \n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "            # Update the learning rate\n",
    "            # scheduler.step(loss)\n",
    "            \n",
    "            _, train_predicted = torch.max(output.data, 1)\n",
    "            train_total += train_label.size(0)\n",
    "            train_correct += (train_predicted == train_label).sum().item()\n",
    "            # print statistics\n",
    "            i += 1\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    loss = running_loss / len(dataloader)\n",
    "    loss_tr.append(loss)\n",
    "    print('[%d, %5d subsamples] Training loss: %.3f' % (epoch + 1, i*BATCH, loss))\n",
    "    running_loss = 0            \n",
    "    # evaluate the model on the validation dataset\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_spectrogram, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                val_label = torch.argmax(val_label, dim=1)\n",
    "            \n",
    "                val_spectrogram = val_spectrogram.squeeze(0)\n",
    "                val_spectrogram = val_spectrogram.unsqueeze(1)\n",
    "                val_spectrogram = val_spectrogram.to(device)\n",
    "                val_output = model(val_spectrogram)\n",
    "                val_loss += loss_fn(val_output, val_label).item()\n",
    "                _, val_predicted = torch.max(val_output.data, 1)\n",
    "                val_total += val_label.size(0)\n",
    "                val_correct += (val_predicted == val_label).sum().item()\n",
    "\n",
    "    loss = val_loss / len(val_dataloader)\n",
    "    loss_val.append(loss)\n",
    "    val_acc = val_correct / val_total\n",
    "    tr_acc = train_correct / train_total \n",
    "    acc_tr.append(tr_acc)\n",
    "    acc_val.append(val_acc)\n",
    "    # Save the model if the validation loss is the best seen so far\n",
    "    if loss < best_val_loss:\n",
    "        best_val_loss = loss\n",
    "        best_val_acc = val_acc\n",
    "        best_tr_acc = tr_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "    print('Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Training Accuracy: {:.4f}'.format(loss, val_acc, tr_acc))\n",
    "\n",
    "plt.plot(loss_val, label='Validation loss')\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_val, label='Validation accuracy')\n",
    "plt.plot(acc_tr, label='Training accuracy')\n",
    "plt.show()\n",
    "\n",
    "torch.save(best_state_dict, model_name + f'_VAL{best_val_acc}_TRAIN{best_tr_acc}.pt')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we keyboard interrupt the training process we can save the best model manually:\n",
    "torch.save(best_state_dict, model_name + f'_VAL{best_val_acc}_TRAIN{best_tr_acc}.pt')\n",
    "# model_name = \"nnet1_2022-01-30_batchsize_32_epochs_100_opt_Adam_LR_0.0001_dropout1_acc0.373_subsampling\"\n",
    "# torch.save(model, model_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"ResNet_2022-01-23_batchsize_40_epochs_5_opt_Adam_LR_0.001\"\n",
    "# model_name = \"nnet1_2022-01-25_batchsize_32_epochs_50_opt_Adam_LR_0.0001_acc_overfitted\" #epoch15 overfit\n",
    "# model_name = \"nnet1_2022-01-30_batchsize_32_epochs_100_opt_Adam_LR_0.0001_dropout1_acc0.373_subsampling\" # no overfit\n",
    "# model_name = \"ResNet_2022-01-23_batchsize_40_epochs_10_opt_Adam_LR_0.001\"\n",
    "torch.load(model_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a Mp3Dataset from a directory of MP3 files\n",
    "dataset = FMADataset(DATA_DIR, train)\n",
    "\n",
    "# create a DataLoader from the FMADataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    \n",
    "# create the CNN model\n",
    "model = ResNet1D(input_size=750000, num_classes=8)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for waveform, label in dataloader:\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "#         waveform = waveform.unsqueeze(0)  # add a batch dimension\n",
    "#         print(waveform.shape)\n",
    "\n",
    "        # extract the first channel\n",
    "        first_channel = waveform[:, :, 0]\n",
    "\n",
    "        # reshape the first channel to add an additional dimension for the channel dimension\n",
    "        first_channel = first_channel.unsqueeze(-1)\n",
    "        \n",
    "        output = model(first_channel)\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        i += 1\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 100 samples\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing - subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# create a dataset object for testing\n",
    "test_dataset = FMADataset(DATA_DIR, test)\n",
    "batch_size = 8\n",
    "# create a data loader to load the dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # don't need to track, calculate or save the gradients in the model\n",
    "    for subsamples, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        batch_size = labels.size(0) # we reupdate the batch size because the last batch can be incomplete.\n",
    "        subsample_outputs = {i: [] for i in range(batch_size)}\n",
    "        for waveform in subsamples:\n",
    "            waveform = waveform.squeeze(0)  \n",
    "            waveform = waveform.unsqueeze(-1)\n",
    "            waveform = waveform.to(device)\n",
    "            outputs = model(waveform)\n",
    "            predicted = torch.argmax(outputs.data, dim=1).cpu()\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                subsample_outputs[j].append(predicted[j]) \n",
    "        for j in range(batch_size):\n",
    "            # count the occurrences of each class\n",
    "            counts = np.bincount(subsample_outputs[j])\n",
    "            # Find the class with the highest count\n",
    "            aggregate_prediction = np.argmax(counts)\n",
    "            correct += (aggregate_prediction == labels[j])\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    \n",
    "        print(f\"CORRECT #  {correct}\")\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing - full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataset object for testing\n",
    "test_dataset = FMADataset(DATA_DIR, test)\n",
    "\n",
    "# create a data loader to load the dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # don't need to track, calculate or save the gradients in the model\n",
    "    for data in test_loader:\n",
    "        # get the inputs\n",
    "        audio, labels = data\n",
    "        # wrap them in a torch Variable\n",
    "        audio, labels = audio.to(device), labels.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        predicted = torch.argmax(outputs.data, dim=1)\n",
    "        print(labels)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(f\"CORRECT #  {correct}\")\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prody/anaconda3/envs/unipd/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "data_dir = './fma/data/fma_small'\n",
    "output_dir = './fma/data/waveforms'\n",
    "sampling_rate = 22_050\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "resample = torchaudio.transforms.Resample(44100, sampling_rate)\n",
    "    \n",
    "# loop through all MP3 files in the data directory\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.mp3'):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            try: \n",
    "                waveform, sample_rate = torchaudio.load(filepath)\n",
    "                waveform = stereo_to_mono(waveform) \n",
    "\n",
    "                # resample the waveform to the desired sample rate using the Resample transform\n",
    "                waveform = resample(waveform)\n",
    "\n",
    "                # save the spectrogram to the output directory\n",
    "                output_file = os.path.join(output_dir, filename[:-4] + '.pt')\n",
    "                torch.save(waveform, output_file)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks on implementations - draft\n",
    "\n",
    "### There are two types of samples: mono and stereo - we need to convert mono to stereo when feeding the CNN\n",
    "\n",
    "An audio channel refers to a single track of audio. The number of channels in an audio file determines the number of separate audio tracks that are mixed together to form the final audio.\n",
    "\n",
    "A mono audio file has a single channel, which means that all the audio is mixed together into one single track. This means that if you play a mono audio file, the same audio will come out of both the left and right speakers (or headphones) and it will sound the same regardless of the stereo or mono setup.\n",
    "\n",
    "A stereo audio file, on the other hand, has two channels - a left channel and a right channel. These two channels carry separate audio tracks that are mixed together to create the final audio. When played back on a stereo setup, each channel will be played through its corresponding speaker or headphone and this way, the stereo audio creates a sense of space and directionality.\n",
    "\n",
    "So, for example, a stereo audio recording of a live concert will have different audio captured by different microphone positioned in different positions in the concert hall, and when it is played back, it creates the sense of being there in the concert hall.\n",
    "\n",
    "It is worth noting that there are also audio file format with more than 2 channels, such as 5.1 or 7.1 surround sound audio.\n",
    "\n",
    "\n",
    "### Downsampling\n",
    "\n",
    " we downsample the audio signals to a lower sample rate to reduce the data size or to simplify the processing of the signal. Downsampling can be useful for tasks such as speech recognition or audio classification, where the lower frequencies of the signal are more important than the higher frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One batch check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH = 1\n",
    "\n",
    "# create a training dataset and dataloader\n",
    "dataset = FMA2D(train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# create a validation dataset and dataloader\n",
    "val_dataset = FMA2D(val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "    \n",
    "# create the CNN model\n",
    "model = nnet1(num_classes=8) # HERE YOU PUT UR NETWORK\n",
    "model.to(device)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# Lee 2017\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "import time\n",
    "    \n",
    "# train the model\n",
    "acc_tr = []\n",
    "acc_val = []\n",
    "loss_tr = []\n",
    "loss_val = []\n",
    "\n",
    "spectrogram, label = next(iter(dataloader))\n",
    "spectrogram = spectrogram.squeeze(-1)  \n",
    "spectrogram = spectrogram.unsqueeze(0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "            # evaluate the model on the training dataset\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "    #for spectrogram, label in dataloader:\n",
    "            \n",
    "            label = label.to(device)\n",
    "            train_label = torch.argmax(label, dim=1)\n",
    "\n",
    "\n",
    "            # forward pass\n",
    "#             spectrogram = spectrogram.squeeze(-1)  \n",
    "#             spectrogram = spectrogram.unsqueeze(0)\n",
    "            \n",
    "            spectrogram = spectrogram.to(device)\n",
    "            output = model(spectrogram)\n",
    "            print(spectrogram.size())\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "            # Update the learning rate\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            _, train_predicted = torch.max(output.data, 1)\n",
    "            train_total += train_label.size(0)\n",
    "            train_correct += (train_predicted == train_label).sum().item()\n",
    "            # print statistics\n",
    "            i += 1\n",
    "            running_loss += loss.item()\n",
    "            print(train_correct)\n",
    "            \n",
    "            \n",
    "            \n",
    "#     loss = running_loss / len(dataloader)\n",
    "#     loss_tr.append(loss)\n",
    "#     print('[%d, %5d subsamples] Training loss: %.3f' % (epoch + 1, i*BATCH, loss))\n",
    "#     running_loss = 0            \n",
    "#     # evaluate the model on the validation dataset\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     val_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for val_spectrogram, val_label in val_dataloader:\n",
    "#                 val_label = val_label.to(device)\n",
    "#                 val_label = torch.argmax(val_label, dim=1)\n",
    "            \n",
    "#                 val_spectrogram = val_spectrogram.squeeze(-1)\n",
    "#                 val_spectrogram = val_spectrogram.unsqueeze(0)\n",
    "#                 val_spectrogram = val_spectrogram.to(device)\n",
    "#                 val_output = model(val_spectrogram)\n",
    "#                 val_loss += loss_fn(val_output, val_label).item()\n",
    "#                 _, val_predicted = torch.max(val_output.data, 1)\n",
    "#                 val_total += val_label.size(0)\n",
    "#                 val_correct += (val_predicted == val_label).sum().item()\n",
    "\n",
    "#     loss = val_loss / len(val_dataloader)\n",
    "#     loss_val.append(loss)\n",
    "#     val_acc = val_correct / val_total\n",
    "#     tr_acc = train_correct / train_total \n",
    "#     acc_tr.append(tr_acc)\n",
    "#     acc_val.append(val_acc)\n",
    "#     print('Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Training Accuracy: {:.4f}'.format(loss, val_acc, tr_acc))\n",
    "\n",
    "plt.plot(loss_val, label='Validation loss')\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_val, label='Validation accuracy')\n",
    "plt.plot(acc_tr, label='Training accuracy')\n",
    "plt.show()\n",
    "    \n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unipd",
   "language": "python",
   "name": "unipd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
