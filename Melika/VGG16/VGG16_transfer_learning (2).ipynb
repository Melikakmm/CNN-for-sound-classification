{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb9b055"
      },
      "source": [
        "# Transfer learning:\n",
        "\n",
        "\n",
        "Transfer learning is a popular technique in deep learning that allows the use of pre-trained neural network models to solve new tasks. By leveraging the knowledge learned from one task, a pre-trained model can be fine-tuned on a different, but related task, thus saving time and computational resources. PyTorch is a popular deep learning framework that provides powerful tools and libraries for building and training neural networks, including pre-trained models that can be used for transfer learning. With PyTorch, transfer learning can be easily implemented, enabling developers to quickly build high-performing models for a wide range of applications."
      ],
      "id": "7fb9b055"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opat09b3MzN2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "opat09b3MzN2"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "44612341"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from tqdm import tqdm\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from torchsummary import summary\n",
        "#Confusion matrix:\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n"
      ],
      "id": "44612341"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1ec184c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler"
      ],
      "id": "1ec184c3"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "stOHWx-cS-q2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "py_file_location = \"/content/drive/MyDrive/\"\n",
        "sys.path.append(os.path.abspath(py_file_location))"
      ],
      "id": "stOHWx-cS-q2"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3a81bc60"
      },
      "outputs": [],
      "source": [
        "\n",
        "import utils"
      ],
      "id": "3a81bc60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80f68f9d"
      },
      "outputs": [],
      "source": [
        "# define device\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ],
      "id": "80f68f9d"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3c0da98e",
        "outputId": "96eb29dc-4eb8-4a9e-a732-2f8cda053d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough Echonest features: (13129, 767)\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/fma/data/fma_small'\n",
        "\n",
        "tracks = utils.load('/content/drive/MyDrive/fma/data/fma_metadata/tracks.csv')\n",
        "features = utils.load('/content/drive/MyDrive/fma/data/fma_metadata/features.csv')#annotation files\n",
        "echonest = utils.load('/content/drive/MyDrive/fma/data/fma_metadata/echonest.csv')\n",
        "\n",
        "subset = tracks.index[tracks['set', 'subset'] <= 'small']\n",
        "\n",
        "assert subset.isin(tracks.index).all()\n",
        "assert subset.isin(features.index).all()\n",
        "\n",
        "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
        "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
        "\n",
        "tracks = tracks.loc[subset]\n",
        "features_all = features.loc[subset]\n",
        "\n",
        "tracks.shape, features_all.shape\n",
        "\n",
        "train = tracks.index[tracks['set', 'split'] == 'training'] #bunch of indexes (not ids) for training val and test\n",
        "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
        "test = tracks.index[tracks['set', 'split'] == 'test']"
      ],
      "id": "3c0da98e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8d43b86"
      },
      "outputs": [],
      "source": [
        "len(train)"
      ],
      "id": "e8d43b86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa2e223"
      },
      "outputs": [],
      "source": [
        "tracks_index = tracks.index\n",
        "tracks_index"
      ],
      "id": "3fa2e223"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3ee3dc53"
      },
      "outputs": [],
      "source": [
        "#From data to one hot labels\n",
        "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
        "labels_onehot_Ten = torch.tensor(labels_onehot)\n",
        "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#from onehot labels to encoded targets.\n",
        "targets = torch.argmax(labels_onehot_Ten, dim=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "3ee3dc53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48b705f6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#creating an annotation dataframe.\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/fma/data/fma_small/checksums', sep='  |/', header = None,\n",
        "                 names = ['id', 'fold', 'songs'], converters={'fold': str})\n",
        "df.index = tracks_index\n",
        "df.loc[5][1]"
      ],
      "id": "48b705f6"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5262d17d"
      },
      "outputs": [],
      "source": [
        "#adding the ch_three attribute/option to create 3 channel spectrogram.\n",
        "\n",
        "#for manual spectogram we used one channel, but for the prepared one from pytorch we used 3 channel spectogram\n",
        "\n",
        "#three channels are simply just a 3 replicas of 1 channel spectrogram.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "#custome dataset class\n",
        "class FMA(Dataset):\n",
        "    def __init__(self, data_dir, track_ids, annotation,\n",
        "                 target_sample_rate, transformation, num_samples, device = False , twoD = False, paper_cut = False):\n",
        "        self.annotation = annotation\n",
        "        self.data_dir = data_dir\n",
        "        self.track_ids = track_ids\n",
        "        self.filenames = os.listdir(data_dir)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.device = device\n",
        "        self.transformation = transformation\n",
        "        if self.device == True :\n",
        "          self.transformation = transformation.to(self.device)\n",
        "        self.twoD = twoD\n",
        "        self.num_samples = num_samples\n",
        "        self.paper_cut = paper_cut\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tid = self.track_ids[index]\n",
        "        filepath = self._get_audio_sample_path(tid)\n",
        "        label = torch.from_numpy(labels_onehot.loc[tid].values).float()\n",
        "        \n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(filepath)\n",
        "        except:\n",
        "            print(filepath)\n",
        "        #be careful all of the sample rates aren't the same(resample)\n",
        "        #waveform --> (2, 10000) #(number of channels, number of samples)\n",
        "        if self.device == True :\n",
        "          waveform = waveform.to(self.device)\n",
        "        waveform = self._resample_if_necessary(waveform, sr)\n",
        "        waveform = self._mix_down_if_necessary(waveform)\n",
        "        #we have to adjust the length of the audio waveforms before the transformation\n",
        "        waveform = self._cut_if_necessary(waveform)\n",
        "        waveform = self._right_pad_if_necessary(waveform)\n",
        "        if self.twoD == True:\n",
        "            waveform = self.transformation(waveform)\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        \n",
        "        if self.paper_cut == True:\n",
        "            waveform = waveform[:, :128, :513]\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        \n",
        "\n",
        "        return waveform, label\n",
        "    \n",
        "    \n",
        "    def _get_audio_sample_path(self, dex):\n",
        "        fold = self.annotation.loc[dex][1]\n",
        "        path = os.path.join(self.data_dir, fold, self.annotation.loc[dex][2])\n",
        "        return path\n",
        "        \n",
        "\n",
        "            \n",
        "    \n",
        "    \n",
        "    \n",
        "    def _cut_if_necessary(self, waveform):\n",
        "        #this method happens before the transformation\n",
        "        if waveform.shape[1] > self.num_samples:\n",
        "            waveform = waveform[:, :self.num_samples]\n",
        "            return waveform\n",
        "        \n",
        "        \n",
        "    def _right_pad_if_necessary(self, waveform):\n",
        "        if waveform.shape[1] < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - waveform.shape[1]\n",
        "            last_dim_padding = (0,num_missing_samples) # (1, 2) -> (left, right)   \n",
        "            #(1, 2, 0, 1) -> (left, right, padnumleft, padnumright)\n",
        "            # what happens is : [1, 1, 1] --> [0, 1, 1, 1, 0, 0]\n",
        "            waveform = torch.nn.functional.pad(waveform, last_dim_padding)\n",
        "            waveform = waveform.T\n",
        "        return waveform\n",
        "    \n",
        "    \n",
        "        \n",
        "    def _resample_if_necessary(self, waveform , sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            waveform = resampler(waveform)\n",
        "        return waveform\n",
        "    \n",
        "    \n",
        "    #from (2, 10000) to (1, 0000) taking the average between two waveforms\n",
        "    def _mix_down_if_necessary(self, waveform):\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform , dim = 0, keepdim = True)\n",
        "        return waveform\n",
        "     \n",
        "    \n",
        "\n",
        "        \n",
        "        return waveform, label \n",
        "    \n",
        "    def __len__(self):#just gives us the number of samples in our datasets.\n",
        "        return len(self.track_ids) \n",
        "\n",
        "        \n",
        "\n",
        "        "
      ],
      "id": "5262d17d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f114c7a1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#trying the class:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "\n",
        "    SAMPLE_RATE=44100\n",
        "\n",
        "    #maxlength\n",
        "    NUM_SAMPLES = 44100\n",
        "    #working on GPU\n",
        " #   if torch.cuda.is_available():\n",
        " #       Device = \"cuda\"\n",
        " #   else:\n",
        " #       Device = \"cpu\"\n",
        "        \n",
        " #   print(f\"we are using {Device}.\")  \n",
        "    \n",
        "    #50% hop_length is the best for accuracy\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate = SAMPLE_RATE, n_fft = 1024, hop_length = 256,\n",
        "                                                        n_mels = 64) \n",
        "    \n",
        "    n_fft = 1024    # FFT window size\n",
        "    hop_length = 256    # number of samples between successive frames\n",
        "    win_length = n_fft\n",
        "    \n",
        "    spectrogram = torchaudio.transforms.Spectrogram(n_fft=1024, hop_length = 256, win_length = win_length )\n",
        "    \n",
        "    \n",
        "    \n",
        "    FL = FMA(DATA_DIR, train, df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True)\n",
        "    print(f\"there are {len(FL)} samples in the dataset\" )\n",
        "    waveform, label = FL[0] #track number 2\n",
        "    a = 1\n",
        "    \n",
        "    \n"
      ],
      "id": "f114c7a1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS0eyRPnVtqc"
      },
      "outputs": [],
      "source": [
        "#Here are corruted songs!\n",
        "#it's your choice how to deal with the :)\n",
        "\n",
        "Dex = tracks_index\n",
        "\n",
        "def _get_audio_sample_path(data_dir, dex): \n",
        "        fold = df.loc[dex][1]\n",
        "        path = os.path.join(data_dir, fold, df.loc[dex][2])\n",
        "        return path\n",
        "    \n",
        "for i in Dex:\n",
        "    p = _get_audio_sample_path(DATA_DIR, i)\n",
        "    try:\n",
        "            w, sr = torchaudio.load(p)\n",
        "    except:\n",
        "            print(p)\n",
        "    "
      ],
      "id": "oS0eyRPnVtqc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7474bcc6"
      },
      "source": [
        "# VGG16 without weights"
      ],
      "id": "7474bcc6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2931b5c8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=False)\n",
        "model.classifier[6] = nn.Sequential(nn.Linear(in_features=4096, out_features=8, bias=True), nn.Dropout(0.5))\n",
        "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model"
      ],
      "id": "2931b5c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03c1c74e"
      },
      "outputs": [],
      "source": [
        "BATCH = 32\n",
        "\n",
        "\n",
        "FL = FMA(DATA_DIR, train, df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "val_dataset = FMA(DATA_DIR, val,df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "\n",
        "\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
        "dataloader = torch.utils.data.DataLoader(FL, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "#model.to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
        "\n",
        "# Define the scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5)\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "i = 0\n",
        "running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "train_acV_no = []\n",
        "val_acV_no = [] \n",
        "\n",
        "\n",
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # evaluate the model on the training dataset\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for waveform, label in dataloader:\n",
        "        #label = label.to(device)\n",
        "        train_label = torch.argmax(label, dim=1)\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        waveform = waveform.squeeze(0)\n",
        "\n",
        "\n",
        "        \n",
        "        #waveform = waveform.to(device)\n",
        "        output = model(waveform)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        loss = loss_fn(output, label)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "            \n",
        "        _, train_predicted = torch.max(output.data, 1)\n",
        "        train_total += train_label.size(0)\n",
        "        train_correct += (train_predicted == train_label).sum().item()\n",
        "        # print statistics\n",
        "        i += 1\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    train_a = train_correct / train_total        \n",
        "    train_acV_no.append(train_a)       \n",
        "    print('[%d, %5d subsamples] Training loss: %.3f' % (epoch + 1, i*BATCH, running_loss / len(dataloader)))\n",
        "    running_loss = 0            \n",
        "    # evaluate the model on the validation dataset\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for val_waveform, val_label in val_dataloader:\n",
        "            #val_label = val_label.to(device)\n",
        "            val_label = torch.argmax(val_label, dim=1)\n",
        "            val_waveform = val_waveform.squeeze(0)\n",
        "            \n",
        "            #val_waveform = val_waveform.to(device)\n",
        "            val_output = model(val_waveform)\n",
        "            val_loss += loss_fn(val_output, val_label).item()\n",
        "            _, val_predicted = torch.max(val_output.data, 1)\n",
        "            val_total += val_label.size(0)\n",
        "            val_correct += (val_predicted == val_label).sum().item()\n",
        "            \n",
        "            \n",
        "    val_a = val_correct/ val_total\n",
        "    val_acV_no.append(val_a)\n",
        "    print('Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Training Accuracy: {:.4f}'.format(val_loss / len(val_dataloader), val_correct / val_total, train_correct / train_total))\n",
        "print('Finished Training')"
      ],
      "id": "03c1c74e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c50e3599"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/train_acV_no.json\", 'w') as f:\n",
        "\n",
        "    json.dump(train_acV_no, f, indent=2) \n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/val_acV_no.json\", 'w') as f:\n",
        "\n",
        "    json.dump(val_acV_no, f, indent=2) "
      ],
      "id": "c50e3599"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75b8ee20"
      },
      "source": [
        "# VGG16 with pytorch pretrained weights:"
      ],
      "id": "75b8ee20"
    },
    {
      "cell_type": "markdown",
      "source": [
        "when using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model."
      ],
      "metadata": {
        "id": "PgBo6WNLGU3W"
      },
      "id": "PgBo6WNLGU3W"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "weights = torchvision.models.VGG16_Weights.DEFAULT\n",
        "model = torchvision.models.vgg16(weights=weights)"
      ],
      "metadata": {
        "id": "BMQS4uhSGdMO"
      },
      "id": "BMQS4uhSGdMO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ZSncbv4Vva2Y"
      },
      "id": "ZSncbv4Vva2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We have to change the first layer, and last layer of the networkbc our data has 1 channel and 8 outputs.\n",
        "#but we are going to freeze other layers.\n",
        "for i in range(1, 31):\n",
        "    for param in model.features[i].parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "for j in range(0, 6):\n",
        "  for param in model.classifier[j].parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "CVrzkPLGu-T7"
      },
      "id": "CVrzkPLGu-T7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the first cnn and last linear layer:\n",
        "\n",
        "model.classifier[6] = nn.Sequential(nn.Linear(in_features=4096, out_features=8, bias=True), nn.Dropout(0.5))\n",
        "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model"
      ],
      "metadata": {
        "id": "nMp1vXrGxT8z"
      },
      "id": "nMp1vXrGxT8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31e1ab91"
      },
      "outputs": [],
      "source": [
        "BATCH = 32\n",
        "\n",
        "\n",
        "FL = FMA(DATA_DIR, train, df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "val_dataset = FMA(DATA_DIR, val,df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "\n",
        "\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
        "dataloader = torch.utils.data.DataLoader(FL, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "#model.to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define the scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5)\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "i = 0\n",
        "running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "train_acV_pyt = []\n",
        "val_acV_pyt = [] \n",
        "\n",
        "\n",
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # evaluate the model on the training dataset\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for waveform, label in dataloader:\n",
        "        #label = label.to(device)\n",
        "        train_label = torch.argmax(label, dim=1)\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        waveform = waveform.squeeze(0)\n",
        "\n",
        "\n",
        "        \n",
        "        #waveform = waveform.to(device)\n",
        "        output = model(waveform)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        loss = loss_fn(output, label)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "            \n",
        "        _, train_predicted = torch.max(output.data, 1)\n",
        "        train_total += train_label.size(0)\n",
        "        train_correct += (train_predicted == train_label).sum().item()\n",
        "        # print statistics\n",
        "        i += 1\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    train_a = train_correct / train_total        \n",
        "    train_acV_pyt.append(train_a)       \n",
        "    print('[%d, %5d subsamples] Training loss: %.3f' % (epoch + 1, i*BATCH, running_loss / len(dataloader)))\n",
        "    running_loss = 0            \n",
        "    # evaluate the model on the validation dataset\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for val_waveform, val_label in val_dataloader:\n",
        "            #val_label = val_label.to(device)\n",
        "            val_label = torch.argmax(val_label, dim=1)\n",
        "            val_waveform = val_waveform.squeeze(0)\n",
        "            \n",
        "            #val_waveform = val_waveform.to(device)\n",
        "            val_output = model(val_waveform)\n",
        "            val_loss += loss_fn(val_output, val_label).item()\n",
        "            _, val_predicted = torch.max(val_output.data, 1)\n",
        "            val_total += val_label.size(0)\n",
        "            val_correct += (val_predicted == val_label).sum().item()\n",
        "            \n",
        "            \n",
        "    val_a = val_correct/ val_total\n",
        "    val_acV_pyt.append(val_a)\n",
        "    print('Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Training Accuracy: {:.4f}'.format(val_loss / len(val_dataloader), val_correct / val_total, train_correct / train_total))\n",
        "print('Finished Training')"
      ],
      "id": "31e1ab91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fdae45"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/train_acV_pyt.json\", 'w') as f:\n",
        "\n",
        "    json.dump(train_acV_pyt, f, indent=2) \n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/val_acV_pyt.json\", 'w') as f:\n",
        "\n",
        "    json.dump(val_acV_pyt, f, indent=2) "
      ],
      "id": "75fdae45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "332c8e8e"
      },
      "source": [
        "# VGG16 with GTZAN pretrained weights:"
      ],
      "id": "332c8e8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60a64455"
      },
      "source": [
        "### GTZAN dataset:\n",
        "The GTZAN dataset is a widely used benchmark dataset in the field of music information retrieval. It was created by George Tzanetakis and Perry Cook in 2002, and is named after Tzanetakis' initials. The dataset consists of 1,000 audio tracks of 30 seconds each, equally divided into 10 different music genres: blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae, and rock. The audio tracks were collected from various online sources and were pre-processed to have a uniform format and sampling rate. The GTZAN dataset has been extensively used for various music classification tasks, such as genre classification, mood detection, and instrument recognition. Its popularity stems from its wide variety of genres and large sample size, which makes it a valuable resource for developing and evaluating music analysis algorithms."
      ],
      "id": "60a64455"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6afb54b9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "model = models.vgg16(pretrained=False)\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Result/model_weights.pth'))\n"
      ],
      "id": "6afb54b9"
    },
    {
      "cell_type": "code",
      "source": [
        "#We have to change the first layer, and last layer of the networkbc our data has 1 channel and 8 outputs.\n",
        "#but we are going to freeze other layers.\n",
        "for i in range(1, 31):\n",
        "    for param in model.features[i].parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "for j in range(0, 6):\n",
        "  for param in model.classifier[j].parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "WAy2MjtwxOuT"
      },
      "id": "WAy2MjtwxOuT",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#changing the first cnn and last linear layer:\n",
        "\n",
        "model.classifier[6] = nn.Sequential(nn.Linear(in_features=4096, out_features=8, bias=True), nn.Dropout(0.5))\n",
        "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model"
      ],
      "metadata": {
        "id": "DJGepIpmxiZQ"
      },
      "id": "DJGepIpmxiZQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "sxQ26Q78XkqA"
      },
      "id": "sxQ26Q78XkqA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 32\n",
        "\n",
        "\n",
        "FL = FMA(DATA_DIR, train, df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "val_dataset = FMA(DATA_DIR, val,df, SAMPLE_RATE, mel_spectrogram, NUM_SAMPLES, twoD =True, paper_cut = True)\n",
        "\n",
        "\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH, shuffle=True)\n",
        "dataloader = torch.utils.data.DataLoader(FL, batch_size=BATCH, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define the scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=5)\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "i = 0\n",
        "running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "train_acV_G = []\n",
        "val_acV_G = [] \n",
        "\n",
        "\n",
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # evaluate the model on the training dataset\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for waveform, label in dataloader:\n",
        "        label = label.to(device)\n",
        "        waveform = waveform.to(device)\n",
        "        train_label = torch.argmax(label, dim=1)\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        waveform = waveform.squeeze(0)\n",
        "\n",
        "\n",
        "        \n",
        "        #waveform = waveform.to(device)\n",
        "        output = model(waveform)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        loss = loss_fn(output, label)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "            \n",
        "        _, train_predicted = torch.max(output.data, 1)\n",
        "        train_total += train_label.size(0)\n",
        "        train_correct += (train_predicted == train_label).sum().item()\n",
        "        # print statistics\n",
        "        i += 1\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        \n",
        "    train_a = train_correct / train_total        \n",
        "    train_acV_G.append(train_a)       \n",
        "    print('[%d, %5d subsamples] Training loss: %.3f' % (epoch + 1, i*BATCH, running_loss / len(dataloader)))\n",
        "    running_loss = 0            \n",
        "    # evaluate the model on the validation dataset\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for val_waveform, val_label in val_dataloader:\n",
        "            val_label = val_label.to(device)\n",
        "            val_label = torch.argmax(val_label, dim=1)\n",
        "            val_waveform = val_waveform.squeeze(0)\n",
        "            \n",
        "            val_waveform = val_waveform.to(device)\n",
        "            val_output = model(val_waveform)\n",
        "            val_loss += loss_fn(val_output, val_label).item()\n",
        "            _, val_predicted = torch.max(val_output.data, 1)\n",
        "            val_total += val_label.size(0)\n",
        "            val_correct += (val_predicted == val_label).sum().item()\n",
        "            \n",
        "            \n",
        "    val_a = val_correct/ val_total\n",
        "    val_acV_G.append(val_a)\n",
        "    print('Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Training Accuracy: {:.4f}'.format(val_loss / len(val_dataloader), val_correct / val_total, train_correct / train_total))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "kpSh9Zeyx0Ju"
      },
      "id": "kpSh9Zeyx0Ju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/train_acV_G.json\", 'w') as f:\n",
        "\n",
        "    json.dump(train_acV_G, f, indent=2) \n",
        "\n",
        "with open(\"/content/drive/MyDrive/Result/val_acV_G.json\", 'w') as f:\n",
        "\n",
        "    json.dump(val_acV_G, f, indent=2) "
      ],
      "metadata": {
        "id": "ZrbFB1mwyGYO"
      },
      "id": "ZrbFB1mwyGYO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}