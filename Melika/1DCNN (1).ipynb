{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prody/anaconda3/envs/unipd/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough Echonest features: (13129, 767)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './fma/data/fma_small'\n",
    "\n",
    "tracks = utils.load('fma/data/fma_metadata/tracks.csv')\n",
    "features = utils.load('fma/data/fma_metadata/features.csv')\n",
    "echonest = utils.load('fma/data/fma_metadata/echonest.csv')\n",
    "\n",
    "subset = tracks.index[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "assert subset.isin(tracks.index).all()\n",
    "assert subset.isin(features.index).all()\n",
    "\n",
    "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "features_all = features.loc[subset]\n",
    "\n",
    "tracks.shape, features_all.shape\n",
    "\n",
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mono_to_stereo(waveform):\n",
    "    # reshape the waveform from a 1D tensor to a 2D tensor with 2 columns\n",
    "    waveform = waveform.view(-1, 1)\n",
    "    # repeat the waveform along the columns to create a stereo signal\n",
    "    waveform = waveform.repeat(1, 2)\n",
    "    return waveform.T\n",
    "\n",
    "def stereo_to_mono(waveform):\n",
    "    if waveform.dim() == 2:\n",
    "        return torch.mean(waveform, dim=0)\n",
    "    elif waveform.dim() == 1:\n",
    "        return waveform\n",
    "    else:\n",
    "        raise ValueError(\"Input must be 1D or 2D tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FMADataset(Dataset):\n",
    "    def __init__(self, data_dir, track_ids, subsampling=True, sampling_rate=22050):\n",
    "        self.data_dir = data_dir\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.track_ids = track_ids\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_length = 750000\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "        # create the Resample transform\n",
    "        self.resample = torchaudio.transforms.Resample(44100, sampling_rate)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tid = self.track_ids[index]\n",
    "        # load the MP3 file\n",
    "        filepath = utils.get_audio_path(self.data_dir, tid)\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "\n",
    "        waveform = stereo_to_mono(waveform) \n",
    "\n",
    "        # resample the waveform to the desired sample rate using the Resample transform\n",
    "        waveform = self.resample(waveform)\n",
    "\n",
    "        \n",
    "        # get label\n",
    "        label = torch.from_numpy(labels_onehot.loc[tid].values).float()\n",
    "        \n",
    "        # subsampling\n",
    "        if self.subsampling:\n",
    "            # set the length of the subsamples and the overlap\n",
    "            subsample_length = self.sampling_rate * 5  # 5 seconds\n",
    "            overlap = int(subsample_length * 0.75)  # 75% overlap\n",
    "            subsamples = []\n",
    "            shift = subsample_length - overlap\n",
    "            for i in range(0, waveform.size(0) - subsample_length + 1, shift):\n",
    "                subsample = waveform[i:(i + subsample_length)]\n",
    "                subsamples.append(subsample)\n",
    "                if len(subsamples) == 20: break\n",
    "            \n",
    "            # Return the subsamples\n",
    "            return subsamples, label\n",
    "        else:\n",
    "            # padding\n",
    "            padding = self.max_length - waveform.shape[0]\n",
    "            padding_tensor = torch.zeros((padding, waveform.shape[1]))\n",
    "            waveform = torch.cat((waveform, padding_tensor), dim=0)\n",
    "\n",
    "        \n",
    "        return waveform, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- modify __getitem__ method of FMADataset class for 2D CNN\n",
    "- implement validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Res1DLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(Res1DLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Projection shortcut\n",
    "        self.projection = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add the projection shortcut\n",
    "        identity = self.projection(identity)\n",
    "        out += identity\n",
    "\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=10):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=3, stride=3, padding=3, bias=False)\n",
    "     \n",
    "        self.layer1 = nn.Sequential(\n",
    "            Res1DLayer(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            Res1DLayer(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            Res1DLayer(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            Res1DLayer(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            Res1DLayer(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            Res1DLayer(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            Res1DLayer(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            Res1DLayer(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            Res1DLayer(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3, padding=1),\n",
    "            nn.Conv1d(512, 512, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        #x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.008\n",
      "[1,    20] loss: 0.008\n",
      "[1,    30] loss: 0.008\n",
      "[1,    40] loss: 0.007\n",
      "[1,    50] loss: 0.008\n",
      "[1,    60] loss: 0.007\n",
      "[1,    70] loss: 0.008\n",
      "[1,    80] loss: 0.007\n",
      "[1,    90] loss: 0.007\n",
      "[1,   100] loss: 0.007\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create a Mp3Dataset from a directory of MP3 files\n",
    "dataset = FMADataset(DATA_DIR, train)\n",
    "\n",
    "# create a DataLoader from the FMADataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    \n",
    "# create the CNN model\n",
    "model = ResNet1D(input_size=110250, num_classes=8)\n",
    "model.to(device)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 1\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for subsamples, label in dataloader:\n",
    "        label = label.to(device)\n",
    "        for waveform in subsamples:\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            waveform = waveform.squeeze(0)  \n",
    "            waveform = waveform.unsqueeze(-1)\n",
    "            \n",
    "            waveform = waveform.to(device)\n",
    "            output = model(waveform)\n",
    "            \n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            i += 1\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 320 samples\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(110250, 128, kernel_size=(3,), stride=(3,), padding=(3,), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Res1DLayer(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (2): Res1DLayer(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (3): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Res1DLayer(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (2): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (3): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (2): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (3): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (5): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (6): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (7): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Res1DLayer(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (projection): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "    (2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.011\n",
      "[1,    20] loss: 0.010\n",
      "[2,    30] loss: 0.010\n",
      "[2,    40] loss: 0.008\n",
      "[3,    50] loss: 0.008\n",
      "[3,    60] loss: 0.009\n",
      "[4,    70] loss: 0.008\n",
      "[4,    80] loss: 0.009\n",
      "[5,    90] loss: 0.008\n",
      "[5,   100] loss: 0.009\n",
      "[6,   110] loss: 0.008\n",
      "[6,   120] loss: 0.008\n",
      "[7,   130] loss: 0.007\n",
      "[7,   140] loss: 0.008\n",
      "[8,   150] loss: 0.008\n",
      "[8,   160] loss: 0.007\n",
      "[9,   170] loss: 0.007\n",
      "[9,   180] loss: 0.007\n",
      "[10,   190] loss: 0.007\n",
      "[10,   200] loss: 0.007\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a Mp3Dataset from a directory of MP3 files\n",
    "dataset = FMADataset(DATA_DIR, train)\n",
    "\n",
    "# create a DataLoader from the FMADataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    \n",
    "# create the CNN model\n",
    "model = ResNet1D(input_size=750000, num_classes=8)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for waveform, label in dataloader:\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "#         waveform = waveform.unsqueeze(0)  # add a batch dimension\n",
    "#         print(waveform.shape)\n",
    "\n",
    "        # extract the first channel\n",
    "        first_channel = waveform[:, :, 0]\n",
    "\n",
    "        # reshape the first channel to add an additional dimension for the channel dimension\n",
    "        first_channel = first_channel.unsqueeze(-1)\n",
    "        \n",
    "        output = model(first_channel)\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        i += 1\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 100 samples\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing - subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT #  5\n",
      "CORRECT #  8\n",
      "CORRECT #  13\n",
      "CORRECT #  17\n",
      "CORRECT #  23\n",
      "Accuracy of the network on the test samples: 14 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# create a dataset object for testing\n",
    "test_dataset = FMADataset(DATA_DIR, test)\n",
    "batch_size = 32\n",
    "# create a data loader to load the dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # don't need to track, calculate or save the gradients in the model\n",
    "    for subsamples, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        batch_size = labels.size(0) # we reupdate the batch size because the last batch can be incomplete.\n",
    "        subsample_outputs = {i: [] for i in range(batch_size)}\n",
    "        for waveform in subsamples:\n",
    "            waveform = waveform.squeeze(0)  \n",
    "            waveform = waveform.unsqueeze(-1)\n",
    "            waveform = waveform.to(device)\n",
    "            outputs = model(waveform)\n",
    "            predicted = torch.argmax(outputs.data, dim=1).cpu()\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                subsample_outputs[j].append(predicted[j]) \n",
    "        for j in range(batch_size):\n",
    "            # count the occurrences of each class\n",
    "            counts = np.bincount(subsample_outputs[j])\n",
    "            # Find the class with the highest count\n",
    "            aggregate_prediction = np.argmax(counts)\n",
    "            correct += (aggregate_prediction == labels[j])\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    \n",
    "        print(f\"CORRECT #  {correct}\")\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing - full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/unipd/lib/python3.9/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# create a dataset object for testing\n",
    "test_dataset = FMADataset(DATA_DIR, test)\n",
    "\n",
    "# create a data loader to load the dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "model.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # don't need to track, calculate or save the gradients in the model\n",
    "    for data in test_loader:\n",
    "        # get the inputs\n",
    "        audio, labels = data\n",
    "        # wrap them in a torch Variable\n",
    "        audio, labels = audio.to(device), labels.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        predicted = torch.argmax(outputs.data, dim=1)\n",
    "        print(labels)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(f\"CORRECT #  {correct}\")\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks on implementations - draft\n",
    "\n",
    "### There are two types of samples: mono and stereo - we need to convert mono to stereo when feeding the CNN\n",
    "\n",
    "An audio channel refers to a single track of audio. The number of channels in an audio file determines the number of separate audio tracks that are mixed together to form the final audio.\n",
    "\n",
    "A mono audio file has a single channel, which means that all the audio is mixed together into one single track. This means that if you play a mono audio file, the same audio will come out of both the left and right speakers (or headphones) and it will sound the same regardless of the stereo or mono setup.\n",
    "\n",
    "A stereo audio file, on the other hand, has two channels - a left channel and a right channel. These two channels carry separate audio tracks that are mixed together to create the final audio. When played back on a stereo setup, each channel will be played through its corresponding speaker or headphone and this way, the stereo audio creates a sense of space and directionality.\n",
    "\n",
    "So, for example, a stereo audio recording of a live concert will have different audio captured by different microphone positioned in different positions in the concert hall, and when it is played back, it creates the sense of being there in the concert hall.\n",
    "\n",
    "It is worth noting that there are also audio file format with more than 2 channels, such as 5.1 or 7.1 surround sound audio.\n",
    "\n",
    "\n",
    "### Downsampling\n",
    "\n",
    " we downsample the audio signals to a lower sample rate to reduce the data size or to simplify the processing of the signal. Downsampling can be useful for tasks such as speech recognition or audio classification, where the lower frequencies of the signal are more important than the higher frequencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unipd",
   "language": "python",
   "name": "unipd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
